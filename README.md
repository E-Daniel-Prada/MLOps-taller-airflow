# ğŸ“Œ **DocumentaciÃ³n del Proyecto: MLOPS-TALLER-AIRFLOW**  

## ğŸ“‚ **Estructura del Proyecto**  

```
MLOPS-TALLER-AIRFLOW/
ï„„ï¸ airflow/                  # ConfiguraciÃ³n y scripts de Apache Airflow
ï„„ï¸ airflow_logs/             # Logs generados por Airflow
ï„„ï¸ api/                      # API para servir modelos y manejar peticiones
ï„„ï¸    â”œâ”€â”€ __pycache__/          # Archivos cachÃ© de Python
ï„„ï¸    â”œâ”€â”€ models/               # Modelos entrenados
ï„„ï¸    â”œâ”€â”€ dockerfile            # Dockerfile para la API
ï„„ï¸    â”œâ”€â”€ main.py               # Punto de entrada de la API
ï„„ï¸    â””â”€â”€ requirements.txt      # Dependencias de la API

ï„„ï¸ app/                      # CÃ³digo principal de la aplicaciÃ³n
ï„„ï¸    â”œâ”€â”€ model/                # Modelos de ML
ï„„ï¸    â”œâ”€â”€ preprocess/           # Preprocesamiento de datos
ï„„ï¸    â”‚   â”œâ”€â”€ clear_db.py       # Limpieza de la base de datos
ï„„ï¸    â”‚   â”œâ”€â”€ dockerfile        # Dockerfile para preprocesamiento
ï„„ï¸    â”‚   â”œâ”€â”€ load_penguins.py  # Carga de datos de pingÃ¼inos
ï„„ï¸    â”‚   â””â”€â”€ preprocess.py     # Script general de preprocesamiento
ï„„ï¸    â”œâ”€â”€ train/                # Entrenamiento del modelo
ï„„ï¸    â”‚   â””â”€â”€ train.py          # Script de entrenamiento

ï„„ï¸ dags/                     # DAGs de Airflow
ï„„ï¸    â”œâ”€â”€ __pycache__/          # CachÃ© de Python
ï„„ï¸    â””â”€â”€ ml_pipeline.py        # Pipeline de Machine Learning en Airflow

ï„„ï¸ env/                      # ConfiguraciÃ³n de entornos
ï„„ï¸ logs/                     # Logs de ejecuciÃ³n

ï„„ï¸ mysql/                    # ConfiguraciÃ³n de MySQL
ï„„ï¸    â””â”€â”€ init.sql              # Script de inicializaciÃ³n de la base de datos

ï„„ï¸ plugins/                  # Plugins de Airflow
ï„„ï¸ .env                      # Variables de entorno
ï„„ï¸ .gitignore                # Archivos a ignorar en Git
ï„„ï¸ docker-compose.yml        # OrquestaciÃ³n de servicios con Docker
ï„„ï¸ README.md                 # DocumentaciÃ³n del proyecto
```

---

## ğŸ”¥ **DescripciÃ³n de Componentes**  

### ğŸ”¹ **1. API (`api/`)**  
Contiene el cÃ³digo necesario para exponer los modelos como servicio web.  

- **`main.py`**: Punto de entrada para la API.  
- **`models/`**: Carpeta donde se almacenan los modelos entrenados.  
- **`dockerfile`**: Archivo para construir la imagen Docker de la API.  
- **`requirements.txt`**: Lista de dependencias de la API.  

Ejemplo de consumo de la API

curl --location 'http://localhost:8000/predict' \
--header 'Content-Type: application/json' \
--data '{
           "bill_length_mm": 39.5,
           "bill_depth_mm": 17.4,
           "flipper_length_mm": 186,
           "body_mass_g": 3800,
           "sex": 1,
           "island": 2
}
'

Respuesta esperada

{
    "prediction": 0
}

---

### ğŸ”¹ **2. AplicaciÃ³n (`app/`)**  
AquÃ­ se encuentran los scripts para preprocesamiento, entrenamiento y gestiÃ³n del modelo de Machine Learning.  

- **ğŸ“‚ `preprocess/`**  
  - `clear_db.py`: Script para limpiar la base de datos antes de entrenar.  
  - `load_penguins.py`: Carga de datos especÃ­ficos para entrenamiento.  
  - `preprocess.py`: TransformaciÃ³n y limpieza de datos.  
  - `dockerfile`: ConfiguraciÃ³n Docker para preprocesamiento.  

- **ğŸ“‚ `train/`**  
  - `train.py`: Entrena un modelo de Machine Learning con los datos procesados.  

---

### ğŸ”¹ **3. DAGs de Airflow (`dags/`)**  
Contiene los DAGs de Apache Airflow para la orquestaciÃ³n del pipeline de ML.  

- **`ml_pipeline.py`**: Define el pipeline completo de ML en Airflow.  

---

### ğŸ”¹ **4. ConfiguraciÃ³n y OrquestaciÃ³n**  

- **`docker-compose.yml`**: Define cÃ³mo se levantan los servicios del proyecto con Docker.  
- **`mysql/init.sql`**: Script de inicializaciÃ³n para la base de datos MySQL.  
- **`logs/`**: Carpeta de logs generados en la ejecuciÃ³n del sistema.  
- **`env/`**: ConfiguraciÃ³n del entorno.  

---

## ğŸ’¥ **CÃ³mo Ejecutar el Proyecto**  

### 1ï¸âƒ£ **Levantar los servicios con Docker**
```bash
docker compose build
docker compose up
```
Luego, accede a Airflow en:  
ğŸ‘‰ **http://localhost:8080**  

